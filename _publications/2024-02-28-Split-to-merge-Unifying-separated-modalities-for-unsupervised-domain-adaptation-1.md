---
title: "Split to merge: Unifying separated modalities for unsupervised domain adaptation"
collection: publications
category: conferences
permalink: /publication/2024-02-28-Split-to-merge-Unifying-separated-modalities-for-unsupervised-domain-adaptation-1
excerpt: 'Unsupervised domain adaptation (UDA) of vision-language models (e.g., CLIP).'
date: 2024-02-28
venue: 'Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)'
paperurl: 'http://xinyao326.github.io/files/24_CVPR_split_to_merge_unifying_separate.pdf'
citation: 'X. Li, Y. Li, Z. Du, F. Li, K. Lu, and J. Li. (2024). &quot;Split to merge: Unifying separated modalities for unsupervised domain adaptation.&quot; <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>.'
---

Large vision-language models (VLMs) like CLIP have demonstrated good zero-shot learning performance in the unsupervised domain adaptation task. Yet, most transfer approaches for VLMs focus on either the language or visual branches, overlooking the nuanced interplay between both modalities. In this work, we introduce a Unified Modality Separation (UniMoS) framework for unsupervised domain adaptation. Leveraging insights from modality gap studies, we craft a nimble modality separation network that distinctly disentangles CLIPâ€™s features into languageassociated and vision-associated components. Our proposed Modality-Ensemble Training (MET) method fosters  the exchange of modality-agnostic information while maintaining modality-specific nuances. We align features across domains using a modality discriminator. Comprehensive evaluations on three benchmarks reveal our approach sets a new state-of-the-art with minimal computational costs.
